{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Time Series Anomaly Detection using AutoEncoders (LSTM) in Pytorch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPTutsZI9lR2cxjz49A5Ors"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"PxnhUFuJQgkJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V6ZAGjAtQhqh"},"source":["Detecting anomalies in real-time time-series is a challenging task. The conventional way has been using some threshold value of the time series and using it to flag an anomaly whenever the time series breached the threshold value. These threshold values can be a static (most basic) or dynamic (calculated by analysing the time series in a relatively small recent time window). However choosing an accurate threshold has been very difficult.\n","\n","Autoencoder networks overcome this challenge by presenting us with an unsupervised method which can calculate the threshold itself. UNder the hood, autoencoders try to reconstruct the original signal that they are fit through a sequence of encoding and decoding units. During this process of reconstruction they try to learn only the most important features or a low dimensional representation of the data by minimizing the reconstruction loss.\n","\n","The architecture as discussed earlier is composed of two important units: the encoder and the decoder. These steps here will be achieved by using LSTM layers. At each epoch, the training process feeds the network with training examples (time series), encodes & decodes it back and evaluates the reconstruction error on the training set. We will minimize the L1 loss between the actual time series and its reconstructed version. We also calculate the reconstruction loss for the validation set to tune our networn\n","\n","Threshold Selection:\n","Once the model learns to encode the time series and decode it back to itself with an accepteble validation loss, we have a distribution of the training losses. We then try to extract the maximum value of reconstruction loss from the distribution and use it to flag anomalies in future."]}]}